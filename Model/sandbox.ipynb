{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sympy import primerange\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "# from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbCallback  # COMMENT OUT WITH OLD VERSION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, ModelMgr_instance):\n",
    "        self.ModelMgr_instance = ModelMgr_instance\n",
    "        self.name_model = 'basemodel'\n",
    "        self.filters = 32\n",
    "        self.n_layers = 3\n",
    "        self.activation = 'relu'\n",
    "        self.dropout = True\n",
    "        self.drop_value = 0.41\n",
    "        self.kernel_size = 3\n",
    "        self.pool_size = 2 # CHANGED FROM 2, SHOULD BE TESTED, ADDED 10-03-2025\n",
    "        self.learning_rate = 0.0001\n",
    "        self.neighborhood_size = self.ModelMgr_instance.neighborhood_size\n",
    "        self.hazard = self.ModelMgr_instance.hazard\n",
    "        self.region = self.ModelMgr_instance.region\n",
    "\n",
    "        if self.ModelMgr_instance.hyper:\n",
    "            self.batch_size = 8192\n",
    "            self.epochs = 5\n",
    "        else:\n",
    "            self.batch_size = 8192 # 2048 256\n",
    "            self.epochs = 5\n",
    "\n",
    "        if self.ModelMgr_instance.test != 'sado':\n",
    "            if os.path.exists(os.path.join(f'Output/{self.region}', self.hazard, f'Sweep_results_BaseModel_{self.ModelMgr_instance.test}.csv')):\n",
    "                df = pd.read_csv(os.path.join(f'Output/{self.region}', self.hazard, f'Sweep_results_BaseModel_{self.ModelMgr_instance.test}.csv'))\n",
    "                row = df.sort_values(by=\"val_loss\", ascending=True).iloc[0]  # \"val_loss\"\n",
    "                self.filters = int(row['filters'])\n",
    "                self.n_layers = int(row['layers'])\n",
    "                self.drop_value = np.round(row['dropout'], 3)\n",
    "                self.learning_rate = np.round(row['lr'], 5)\n",
    "        self.ModelMgr_instance.logger.info(f\"fi:{self.filters} ly:{self.n_layers} dv:{self.drop_value} lr:{self.learning_rate}\")\n",
    "    \n",
    "\n",
    "    # TODO - convert to PyTorch ##############################\n",
    "\n",
    "    def design_basemodel(self):\n",
    "        def safe_binary_crossentropy(y_true, y_pred):\n",
    "            # Handle potential NaN inputs\n",
    "            y_pred = tf.where(tf.math.is_nan(y_pred), tf.zeros_like(y_pred), y_pred)\n",
    "            y_true = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)\n",
    "            # Clip predictions to avoid numerical instability\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "            return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        def safe_mse(y_true, y_pred):\n",
    "            # Handle potential NaN inputs explicitly\n",
    "            y_pred = tf.where(tf.math.is_nan(y_pred), tf.zeros_like(y_pred), y_pred)\n",
    "            y_true = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)\n",
    "            # Clip predictions to avoid numerical instability\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "            return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "            # return tf.keras.metrics.mean_squared_error(y_true, y_pred)  # OLD VERSION\n",
    "        def safe_mae(y_true, y_pred):\n",
    "            # Handle potential NaN inputs explicitly\n",
    "            y_pred = tf.where(tf.math.is_nan(y_pred), tf.zeros_like(y_pred), y_pred)\n",
    "            y_true = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)\n",
    "            # Clip predictions to avoid numerical instability\n",
    "            y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "            # Debugging: print if NaNs appear\n",
    "            tf.debugging.check_numerics(y_pred, \"NaN in predictions\")\n",
    "            tf.debugging.check_numerics(y_true, \"NaN in true values\")\n",
    "            return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "            # return tf.keras.metrics.mean_absolute_error(y_true, y_pred)  # OLD VERSION\n",
    "\n",
    "        # Define the model architecture\n",
    "        self.base_model = self.CNN()\n",
    "\n",
    "        # Compile the model\n",
    "        self.ModelMgr_instance.logger.info('Compiling model')\n",
    "        # optimizer = Adam(learning_rate=self.learning_rate, clipnorm=1.0, epsilon=1e-7)\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate, clipnorm=1.0, epsilon=1e-8) # , clipvalue=1.0\n",
    "        # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9, nesterov=True, clipnorm=1.0, clipvalue=0.5)\n",
    "        self.base_model.compile(optimizer=optimizer, loss=safe_binary_crossentropy, metrics=[safe_mse, safe_mae]) \n",
    "\n",
    "        # Provide model summary\n",
    "        self.base_model.summary()\n",
    "\n",
    "    \n",
    "    # TODO - convert to PyTorch ##############################\n",
    "\n",
    "    def CNN(self):\n",
    "        def safe_sigmoid(x):\n",
    "            return tf.keras.activations.sigmoid(tf.clip_by_value(x, -15.0, 15.0))\n",
    "        self.ModelMgr_instance.logger.info('Building architecture')\n",
    "        # Define the model architecture\n",
    "        input_shape = (self.ModelMgr_instance.input_data[0].shape[1], self.ModelMgr_instance.input_data[0].shape[2], 1)  # Define the input shape for the Conv2D layer\n",
    "\n",
    "        merge_list, input_list = [], []  # Initialize lists for merging and input data\n",
    "        for i, var in enumerate(self.ModelMgr_instance.variables):  # Iterate through the variables\n",
    "            cnn_input = keras.Input(shape=input_shape, name=f'input_{i+1}')  # Define the input layer\n",
    "\n",
    "            # Spatial attention after for resubmission.\n",
    "            cnn1 = layers.Conv2D(self.filters, kernel_size=(self.kernel_size, self.kernel_size), padding='same', activation=self.activation, \n",
    "                                 kernel_initializer=he_normal(seed=42), kernel_regularizer=tf.keras.regularizers.l2(1e-4))(cnn_input)  # Apply Conv2D layer\n",
    "            cnn1 = SpatialAttentionLayer()(cnn1)\n",
    "            cnn1 = layers.MaxPooling2D(pool_size=(self.pool_size, self.pool_size), padding=\"same\")(cnn1)\n",
    "            \n",
    "            for i in range(self.n_layers - 1):  # Iterate through additional convolutional layers\n",
    "                cnn1 = layers.Conv2D(self.filters * 2, kernel_size=(self.kernel_size, self.kernel_size), padding='same', activation=self.activation,\n",
    "                                     kernel_initializer=he_normal(seed=42), kernel_regularizer=tf.keras.regularizers.l2(1e-4))(cnn1)  # Apply Conv2D layer\n",
    "                if i == 1 or i == 3 or i == self.n_layers - 1:\n",
    "                    cnn1 = layers.MaxPooling2D(pool_size=(self.pool_size, self.pool_size), padding=\"same\")(cnn1)  # Apply MaxPooling2D layer\n",
    "                \n",
    "            if len(self.ModelMgr_instance.variables) > 1:  # Check if there are multiple variables\n",
    "                merge_list.append(cnn1)  # Append to the merge list\n",
    "                input_list.append(cnn_input)  # Append to the input list\n",
    "            else:\n",
    "                merge_list = cnn1  # Set the merge list\n",
    "                input_list = cnn_input  # Set the input list\n",
    "\n",
    "        if len(self.ModelMgr_instance.variables) > 1:  # Check if there are multiple variables\n",
    "            merge_list = layers.concatenate(merge_list)  # Concatenate the merge list\n",
    "        # merge_list = layers.Flatten()(merge_list)\n",
    "        merge_list = layers.GlobalAveragePooling2D()(merge_list) # instead of line flatten above \n",
    "        x = layers.Dense(1024, kernel_initializer=he_normal(seed=42), # 1024 128  activation=self.activation, \n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge_list)  # self.filters * 2 \n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(self.activation)(x) \n",
    "        if self.dropout:  # Check if dropout is enabled\n",
    "            x = layers.Dropout(self.drop_value)(x)  # Apply Dropout layer\n",
    "        outputs = layers.Dense(1, kernel_regularizer=regularizers.l2(0.00001), kernel_initializer=he_normal(seed=42), activation='sigmoid')(x)  # Add the output layer\n",
    "\n",
    "        model = keras.Model(inputs=input_list, outputs=outputs, name=self.name_model)  # Define the model\n",
    "\n",
    "        return model\n",
    "\n",
    "    # TODO - convert to PyTorch ##############################\n",
    "\n",
    "    def train(self):\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)  # , start_from_epoch=3\n",
    "        # callbacks = [wandb.keras.WandbCallback(), early_stopping, TerminateOnNaN()]   # OLD VERSION\n",
    "        callbacks=[WandbMetricsLogger(), early_stopping, TerminateOnNaN()]  # , WandbModelCheckpoint(\"models\") WandbMetricsLogger() WandbCallback() CustomWandbCallback()\n",
    "\n",
    "        # Train the model\n",
    "        self.ModelMgr_instance.logger.info('Fitting model')\n",
    "        if self.ModelMgr_instance.partition == 'random':\n",
    "            history = self.base_model.fit({'input_' + str(i+1): self.ModelMgr_instance.model_inputs[i] for i in range(len(self.ModelMgr_instance.model_inputs))},\n",
    "                                        self.ModelMgr_instance.model_labels, validation_split=0.2, epochs=self.epochs, batch_size=self.batch_size, shuffle=True,\n",
    "                                        callbacks=[wandb.keras.WandbCallback(), early_stopping, TerminateOnNaN()])\n",
    "        elif self.ModelMgr_instance.partition == 'spatial':\n",
    "            history = self.base_model.fit({'input_' + str(i+1): self.ModelMgr_instance.model_inputs[i] for i in range(len(self.ModelMgr_instance.model_inputs))},\n",
    "                                        self.ModelMgr_instance.model_labels, epochs=self.epochs, batch_size=self.batch_size, shuffle=True,\n",
    "                                        validation_data=({'input_' + str(i+1): self.ModelMgr_instance.val_data[i] for i in range(len(self.ModelMgr_instance.val_data))}, self.ModelMgr_instance.val_labels),\n",
    "                                        callbacks=callbacks)  # , TerminateOnNaN()  GradientMonitorCallback\n",
    "\n",
    "        fig = self.ModelMgr_instance.plot_val_loss(history, name=f'{self.ModelMgr_instance.test} {self.hazard} Loss base model')\n",
    "        wandb.log({\"Loss_val\": wandb.Image(fig)})\n",
    "        wandb.log({\"bce_val\": history.history['val_loss'][-1]})\n",
    "        self.bce_val = history.history['val_loss'][-1]\n",
    "\n",
    "        # if self.ModelMgr_instance.hyper:\n",
    "        #     if self.bce_val < self.bce_val_best:\n",
    "\n",
    "        for var in self.base_model.trainable_variables:\n",
    "            tf.print(var.name, \"max weight:\", tf.reduce_max(var), \"min weight:\", tf.reduce_min(var), summarize=10)\n",
    "        \n",
    "    \n",
    "    # TODO - convert to PyTorch ##############################\n",
    "    def predict(self):\n",
    "        # Evaluate the model on the validation data\n",
    "        self.ModelMgr_instance.logger.info('Predicting')\n",
    "        susceptibility = self.base_model.predict({'input_' + str(i+1): self.ModelMgr_instance.input_data[i] for i in range(len(self.ModelMgr_instance.input_data))})\n",
    "       \n",
    "        self.ModelMgr_instance.logger.info('Reshaping output')\n",
    "        susc_map = np.zeros(self.ModelMgr_instance.original_shape)\n",
    "        # Fill the new_data_array with the values at the specified indices\n",
    "        for i, index in enumerate(self.ModelMgr_instance.indices_with_values):\n",
    "            susc_map[index, :, :] = susceptibility[i]\n",
    "\n",
    "        susc_shape = (self.ModelMgr_instance.output_shape[0] - self.neighborhood_size*2, self.ModelMgr_instance.output_shape[1] - self.neighborhood_size*2)\n",
    "        susc_map = susc_map.reshape(susc_shape)\n",
    "\n",
    "        # Create a new array with shape of output and fill it with zeros\n",
    "        susc_map_reshape = np.zeros(self.ModelMgr_instance.output_shape)\n",
    "\n",
    "        # Copy the original array into the center of the new array\n",
    "        susc_map_reshape[self.neighborhood_size:self.neighborhood_size+susc_shape[0], self.neighborhood_size:self.neighborhood_size+susc_shape[1]] = susc_map\n",
    "\n",
    "        # Plot the susceptibility map\n",
    "        self.ModelMgr_instance.logger.info('plotting and saving')\n",
    "        fig = self.ModelMgr_instance.plot(susc_map_reshape, name=f'{self.ModelMgr_instance.test} {self.hazard} Susceptibility base model')\n",
    "        np.save(f'Output/{self.region}/{self.hazard}/{self.ModelMgr_instance.test}_{self.hazard}_Susceptibility_base_model.npy', susc_map_reshape)\n",
    "        np.save(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_base_model_rnd_ind_{self.ModelMgr_instance.test}.npy', self.ModelMgr_instance.train_indices)\n",
    "\n",
    "        # Estimate feature importance\n",
    "        if not self.ModelMgr_instance.hyper:\n",
    "            # Compute the minimum and maximum values of the array\n",
    "            self.min_value = np.min(susceptibility)\n",
    "            self.max_value = np.max(susceptibility)\n",
    "\n",
    "            # Perform min-max scaling to scale the array between 0 and 1\n",
    "            self.susceptibility = (susceptibility - self.min_value) / (self.max_value - self.min_value)\n",
    "            self.baseline_accuracy = accuracy_score(np.squeeze(self.ModelMgr_instance.labels, axis=2), (self.susceptibility > 0.5).astype(int))\n",
    "            # self.permutation_feature_importance()\n",
    "\n",
    "        # Save the base model weights\n",
    "        # self.base_model.save(os.path.join(f'Output/{region}', self.hazard, f'base_model_{self.ModelMgr_instance.test}.tf'), save_format='tf') # OLD VERSION\n",
    "        self.base_model.save(os.path.join(f'Output/{self.region}', self.hazard, f'base_model_{self.ModelMgr_instance.test}.keras'))\n",
    "        wandb.log({\"Susceptibility basemodel\": wandb.Image(fig)})\n",
    "\n",
    "# TODO - convert to PyTorch ##############################\n",
    "    def permutation_feature_importance(self):\n",
    "        self.ModelMgr_instance.logger.info('Feature Importance')\n",
    "        # Compute permutation feature importance for each input feature set\n",
    "        num_features = len(self.ModelMgr_instance.input_data)\n",
    "        feature_importances = {}\n",
    "        feature_importances['Baseline'] = self.baseline_accuracy\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            # Copy the original input data for the selected feature set\n",
    "            shuffled_input_data = self.ModelMgr_instance.input_data.copy()\n",
    "\n",
    "            # Shuffle the feature values (permutation)\n",
    "            shuffled_input_data[feature_index] = np.random.shuffle(shuffled_input_data[feature_index])\n",
    "\n",
    "            # Compute model predictions with shuffled feature values\n",
    "            predictions = self.base_model.predict({'input_' + str(i+1): shuffled_input_data[i] for i in range(len(shuffled_input_data))})\n",
    "            predictions = (predictions - self.min_value) / (self.max_value - self.min_value)\n",
    "            shuffled_accuracy = accuracy_score(np.squeeze(self.ModelMgr_instance.labels, axis=2), (predictions > 0.5).astype(int))\n",
    "\n",
    "            # Compute permutation feature importance\n",
    "            permutation_importance = self.baseline_accuracy - shuffled_accuracy\n",
    "            feature_importances[self.ModelMgr_instance.variables[feature_index]] = permutation_importance\n",
    "        \n",
    "        # Save to dataframe\n",
    "        df = pd.DataFrame.from_dict(feature_importances, orient='index')\n",
    "        df.to_excel(f\"Output/{self.region}/{self.hazard}/Permutation_Importance_{self.ModelMgr_instance.test}.xlsx\")\n",
    "\n",
    "# TODO - convert to PyTorch ##############################\n",
    "    def testing(self):\n",
    "        # Evaluate the model on the validation data\n",
    "        self.ModelMgr_instance.logger.info('Testing')\n",
    "\n",
    "        y_pred = self.base_model.predict({'input_' + str(i+1): self.ModelMgr_instance.test_data[i] for i in range(len(self.ModelMgr_instance.test_data))})\n",
    "        y_pred = np.squeeze(y_pred, axis=(1))\n",
    "        y_true = np.squeeze(self.ModelMgr_instance.test_labels, axis=(1,2))\n",
    "\n",
    "        # Calculate Binary Cross-Entropy\n",
    "        y_pred_tf = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        y_true_tf = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true_tf, y_pred_tf)\n",
    "        self.bce_test = tf.reduce_mean(bce).numpy()\n",
    "        self.ModelMgr_instance.logger.info(f\"BCE: {self.bce_test}\")\n",
    "\n",
    "        # Metrics\n",
    "        self.mae = mean_absolute_error(y_true, y_pred)\n",
    "        self.mse = mean_squared_error(y_true, y_pred)\n",
    "        self.ModelMgr_instance.logger.info(f\"MAE: {self.mae}\")\n",
    "        self.ModelMgr_instance.logger.info(f\"MSE: {self.mse}\")\n",
    "\n",
    "        # Create a dictionary to store values with names\n",
    "        metrics_dict = {'MAE': [self.mae], 'MSE': [self.mse]}\n",
    "        df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "        # Write the values to a text file\n",
    "        df.to_csv(f'Output/{self.region}/{self.hazard}/config_{self.ModelMgr_instance.test}_basemodel.csv', index=False)\n",
    "\n",
    "        # Store in W&B\n",
    "        wandb.log({\"MAE_test\": self.mae})\n",
    "        wandb.log({\"MSE_test\": self.mse})\n",
    "        wandb.log({\"BCE_test\": self.bce_test})\n",
    "\n",
    "# TODO - convert to PyTorch ##############################\n",
    "    def HypParOpt(self):\n",
    "        # Set seed\n",
    "        np.random.seed(self.ModelMgr_instance.seed)\n",
    "        tf.random.set_seed(self.ModelMgr_instance.seed)\n",
    "        self.bce_val_best = 1000\n",
    "\n",
    "        # Define sweep config\n",
    "        sweep_configuration = {\n",
    "            \"method\": \"bayes\",\n",
    "            \"name\": f\"{self.ModelMgr_instance.test}_BaseModel\",\n",
    "            \"metric\": {\"goal\": \"minimize\", \"name\": \"bce_val\"}, # \"val_loss\"\n",
    "            \"parameters\": {\n",
    "                \"layers\": {\"values\": [3, 4, 5]},\n",
    "                \"filters\": {\"values\": [32, 64, 96, 128]},\n",
    "                \"lr\": {\"max\": 0.001, \"min\": 0.00001},\n",
    "                \"dropout\": {\"max\": 0.5, \"min\": 0.1},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Initialize sweep by passing in config.\n",
    "        sweep_id = wandb.sweep(sweep=sweep_configuration, project=f\"{self.hazard}-sweep\")\n",
    "        self.hyper_df = pd.DataFrame(columns=[\"layers\", \"filters\", \"lr\", \"dropout\", \"MAE\", \"MSE\", \"val_loss\"], dtype=float)\n",
    "\n",
    "        # Start sweep job.\n",
    "        wandb.agent(sweep_id, function=self.main, count=20)\n",
    "\n",
    "# TODO - convert to PyTorch ##############################\n",
    "    def run(self):\n",
    "        # Set seed\n",
    "        np.random.seed(self.ModelMgr_instance.seed)\n",
    "        tf.random.set_seed(self.ModelMgr_instance.seed)\n",
    "\n",
    "        wandb.init(project=self.hazard, entity=\"timothy-tiggeloven\", group=self.ModelMgr_instance.test, job_type=\"BaseModel\",\n",
    "                config={\"learning_rate\": self.learning_rate,\n",
    "                        \"epochs\": self.epochs,\n",
    "                        \"filters\": self.filters,\n",
    "                        \"layers\": self.n_layers,\n",
    "                        \"seed\": self.ModelMgr_instance.seed,\n",
    "                        \"dropout\": self.drop_value,\n",
    "                        \"NN_cells\": self.neighborhood_size,\n",
    "                        \"variables\": self.ModelMgr_instance.variables,\n",
    "                        \"sample_ratio\": self.ModelMgr_instance.sample_ratio})\n",
    "\n",
    "        # Develop a basemodel\n",
    "        self.main()\n",
    "\n",
    "        # Store Weights and Biases\n",
    "        wandb.finish()\n",
    "    \n",
    "    def main(self):\n",
    "        if self.ModelMgr_instance.hyper:\n",
    "            self.base_model = False\n",
    "            if self.ModelMgr_instance.partition == 'random':\n",
    "                self.ModelMgr_instance.preprocess() \n",
    "            wandb.init()\n",
    "            self.n_layers = wandb.config.layers\n",
    "            self.filters = wandb.config.filters\n",
    "            self.learning_rate = wandb.config.lr\n",
    "            self.drop_value = wandb.config.dropout\n",
    "\n",
    "        # Develop a basemodel\n",
    "        self.design_basemodel()\n",
    "        self.train()\n",
    "        self.predict()\n",
    "        self.testing()\n",
    "\n",
    "        if self.ModelMgr_instance.hyper:\n",
    "            new_row = pd.DataFrame([{\n",
    "                \"layers\": wandb.config.layers,\n",
    "                \"filters\": wandb.config.filters,\n",
    "                \"lr\": wandb.config.lr,\n",
    "                \"dropout\": wandb.config.dropout,\n",
    "                \"val_loss\": self.bce_val,\n",
    "                \"MAE\": self.mae,\n",
    "                \"MSE\": self.mse,\n",
    "            }])\n",
    "            self.hyper_df = pd.concat([self.hyper_df, new_row], ignore_index=True)\n",
    "            self.hyper_df.to_csv(f\"Output/{self.region}/{self.hazard}/Sweep_results_BaseModel_{self.ModelMgr_instance.test}.csv\", index=False)\n",
    "            if self.bce_val < self.bce_val_best:\n",
    "                self.bce_val_best = self.bce_val\n",
    "        self.ModelMgr_instance.logger.info(f\"Main done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMgr:\n",
    "    def __init__(self, region='Europe', test='Europe', prep='model', hazard='Landslide', hyper=False, model_choice='base', partition='spatial'):\n",
    "        self.hazard = hazard\n",
    "        self.region = region\n",
    "        self.name_model = 'susceptibility'\n",
    "        self.missing_data_value = 0\n",
    "        self.sample_ratio = 0.8\n",
    "        self.test_split = 0.15\n",
    "        self.neighborhood_size = 5\n",
    "        self.hyper = hyper\n",
    "        self.test = test\n",
    "        self.model_choice = model_choice\n",
    "        self.partition = partition\n",
    "        if self.hazard == 'Landslide':\n",
    "            self.variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux', 'HWSD', 'road', 'GEM', 'curvature', 'GLIM']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous', 'categorical', 'label', 'continuous', 'continuous', 'categorical']\n",
    "            # self.variables = ['elevation', 'slope', 'landcover', 'NDVI', 'precipitation', 'HWSD']\n",
    "            # self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'categorical']\n",
    "        elif self.hazard == 'Flood':\n",
    "            self.variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        elif self.hazard == 'Tsunami':\n",
    "            self.variables = ['elevation', 'coastlines', 'GEM']\n",
    "            self.var_types = ['continuous', 'continuous', 'continuous']\n",
    "            # self.variables = ['coastlines']\n",
    "            # self.var_types = ['continuous']\n",
    "        elif self.hazard == \"Wildfire\":\n",
    "            # temperature_daily, NDVI, landcover, elevation, wind_speed, fire_weather, soil_moisture(root or surface)\n",
    "            self.variables = ['temperature_daily', 'NDVI', 'landcover', 'elevation', 'wind_speed', 'fire_weather', 'root_soil_moisture']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        elif self.hazard == 'Multihazard':\n",
    "            # self.variables = ['drought', 'extreme_wind', 'fire_weather', 'heatwave', 'pga', 'volcano', 'Flood_base_model', 'Landslide_base_model', 'Tsunami_base_model']\n",
    "            self.variables = ['drought', 'extreme_wind', 'fire_weather', 'heatwave', 'jshis', 'volcano', 'Flood_base_model', 'Landslide_base_model', 'Tsunami_base_model']\n",
    "            self.var_types = ['continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        self.prep = prep\n",
    "        self.ensemble_nr = 5  # 5\n",
    "        self.seed = 43\n",
    "\n",
    "        self.logger, self.ch = self.set_logger()\n",
    "\n",
    "\n",
    "\n",
    "       # PyTorch GPU configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.logger.info(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.logger.info(f\"Num GPUs Available: {torch.cuda.device_count()}\")\n",
    "            self.logger.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        self.logger.info(f\"Torch version: {torch.__version__}\")\n",
    "    \n",
    "        # # Configure memory growth for both GPUs to avoid memory errors\n",
    "        # for gpu in physical_devices:\n",
    "        #     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        # self.logger.info(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "        # Test simple GPU operation\n",
    "        a = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=self.device)\n",
    "        b = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=self.device)\n",
    "        c = torch.matmul(a, b)\n",
    "        self.logger.info(f\"Matrix multiplication result: {c}\")\n",
    "\n",
    "        # sys.exit(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.base_model_instance = BaseModel(self)\n",
    "        # self.ensemble_model_instance = EnsembleModel(self)\n",
    "        # self.meta_model_instance = MetaModel(self)\n",
    "\n",
    "        # if not (self.hyper and self.partition == 'random'):\n",
    "        #     self.preprocess()\n",
    "        # # self.preprocess()\n",
    "\n",
    "    def set_logger(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Set-up the logging system, exit if this fails\n",
    "        \"\"\"\n",
    "        # assign logger file name and output directory\n",
    "        datelog = time.ctime()\n",
    "        datelog = datelog.replace(':', '_')\n",
    "        reference = f'CNN_ls_susc_{self.test}'\n",
    "\n",
    "        logfilename = ('logger' + os.sep + reference + '_logfile_' + \n",
    "                    str(datelog.replace(' ', '_')) + '.log')\n",
    "\n",
    "        # create output directory if not exists\n",
    "        if not os.path.exists('logger'):\n",
    "            os.makedirs('logger')\n",
    "\n",
    "        # create logger and set threshold level, report error if fails\n",
    "        try:\n",
    "            logger = logging.getLogger(reference)\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        except IOError:\n",
    "            sys.exit('IOERROR: Failed to initialize logger with: ' + logfilename)\n",
    "\n",
    "        # set formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s -'\n",
    "                                    '%(levelname)s - %(message)s')\n",
    "\n",
    "        # assign logging handler to report to .log file\n",
    "        ch = logging.handlers.RotatingFileHandler(logfilename,\n",
    "                                                maxBytes=10*1024*1024,\n",
    "                                                backupCount=5)\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "\n",
    "        # assign logging handler to report to terminal\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.DEBUG)\n",
    "        console.setFormatter(formatter)\n",
    "        logger.addHandler(console)\n",
    "\n",
    "        # start up log message\n",
    "        logger.info('File logging to ' + logfilename)\n",
    "\n",
    "        return logger, ch\n",
    "\n",
    "    def preprocess(self):\n",
    "        # Load data from .npy files\n",
    "        # Prepare data for the CNN\n",
    "        input_data = []\n",
    "        spatial_split = False\n",
    "        if self.prep == 'model':\n",
    "            for var, var_type in zip(self.variables, self.var_types):\n",
    "                input_data.append(self.load_normalize(var, var_type=var_type))\n",
    "        elif self.prep == 'stack':\n",
    "            for i in range(self.ensemble_nr):\n",
    "                input_data.append(self.load_normalize(f'model_{i}', var_type='label', crop=False)[0])\n",
    "        elif self.prep == 'multi':\n",
    "            for var, var_type in zip(self.variables, self.var_types):\n",
    "                input_data.append(self.load_normalize(var, var_type=var_type))\n",
    "        elevation = self.load_normalize('elevation', var_type='mask')\n",
    "        input_data = np.array(input_data)\n",
    "\n",
    "        if self.hazard == 'Landslide':\n",
    "            labels, output_shape, spatial_split = self.load_normalize('ldm', var_type='label')\n",
    "        elif self.hazard == 'Flood':\n",
    "            labels, output_shape, spatial_split = self.load_normalize('flood_surge', var_type='label')\n",
    "        elif self.hazard == 'Tsunami':\n",
    "            labels, output_shape, spatial_split = self.load_normalize('tsunami', var_type='label')\n",
    "        elif self.hazard == 'Multihazard':\n",
    "            labels, output_shape = self.load_normalize('multi_hazard', var_type='continuous')\n",
    "\n",
    "        # List to store the indices\n",
    "        self.logger.info('Extracting indices')\n",
    "        indices_with_values = []\n",
    "        original_shape = labels.shape\n",
    "        self.logger.info(f\"Input shape: {input_data.shape}\")\n",
    "        self.logger.info(f\"Label shape: {labels.shape}\")\n",
    "        if spatial_split is not False:\n",
    "            self.logger.info(f\"Spatial shape: {spatial_split.shape}\")\n",
    "        self.logger.info(f\"Elevation shape: {elevation.shape}\")\n",
    "\n",
    "        # Iterate over the array   ############## THIS SHOULD BE DONE IN LOAD NORMALIZE\n",
    "        for idx, data_map in enumerate(elevation):\n",
    "            if np.any(data_map > -9999): ###### SO WOULD NOT BE BETTER TO CHECK ALL MAPS AND MAKE NODATA=0???? FOR MIN MAX SCALER***\n",
    "                indices_with_values.append(idx)\n",
    "\n",
    "        # Extract data based on the indices\n",
    "        input_data = input_data[:, indices_with_values]\n",
    "        labels = labels[indices_with_values]\n",
    "        if spatial_split is not False:\n",
    "            spatial_split = spatial_split[indices_with_values]\n",
    "\n",
    "        self.logger.info(f\"Min value INPUT: {np.min(input_data)}\")\n",
    "        self.logger.info(f\"Max value INPUT: {np.max(input_data)}\")\n",
    "        self.logger.info(f\"Min value LABEL: {np.min(labels)}\")\n",
    "        self.logger.info(f\"Max value LABEL: {np.max(labels)}\")\n",
    "        self.logger.info(f\"Input shape: {input_data.shape}\")\n",
    "        self.logger.info(f\"Label shape: {labels.shape}\")\n",
    "        if spatial_split is not False:\n",
    "            self.logger.info(f\"Spatial shape: {spatial_split.shape}\")\n",
    "\n",
    "        # for i in range(len(input_data)):\n",
    "        #     variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux', 'HWSD', 'road', 'GEM', 'curvature', 'GLIM']\n",
    "        #     self.logger.info(f\"Variable: {variables[i]}\")\n",
    "        #     self.logger.info(f\"Min value INPUT: {np.min(input_data[i])}\")\n",
    "        #     self.logger.info(f\"Max value INPUT: {np.max(input_data[i])}\")\n",
    "        # sys.exit(0)\n",
    "\n",
    "        if self.partition == 'random':\n",
    "            # Generate random indices from the first axis\n",
    "            if not os.path.exists(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy') or self.hyper:\n",
    "                train_indices = random.sample(range(input_data.shape[1]), int(input_data.shape[1] * self.sample_ratio))\n",
    "                train_indices = np.save(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy', train_indices)\n",
    "\n",
    "            train_indices = np.load(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy')\n",
    "\n",
    "            # Create the test set of indices\n",
    "            all_indices = set(range(input_data.shape[1]))\n",
    "            complement_indices = list(all_indices - set(train_indices))\n",
    "\n",
    "            test_indices = random.sample(complement_indices, int(input_data.shape[1] * self.test_split))\n",
    "        elif self.partition == 'spatial':\n",
    "            if not os.path.exists(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy'):\n",
    "                self.logger.info('INDICES')\n",
    "                train_indices = np.where(spatial_split == 1)[0]\n",
    "                self.logger.info(train_indices.shape)\n",
    "                val_indices = np.where(spatial_split == 2)[0]\n",
    "                self.logger.info(val_indices.shape)\n",
    "                test_indices = np.where(spatial_split == 3)[0]\n",
    "                self.logger.info(test_indices.shape)\n",
    "                other_indices = np.where(spatial_split == 0)[0]\n",
    "                self.logger.info(other_indices.shape)\n",
    "                \n",
    "                train_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy', train_indices)\n",
    "                val_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_val.npy', val_indices)\n",
    "                test_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_test.npy', test_indices)\n",
    "            \n",
    "            train_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy')\n",
    "            val_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_val.npy')\n",
    "            test_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_test.npy')\n",
    "\n",
    "        self.input_data = input_data\n",
    "        self.labels = labels\n",
    "\n",
    "        # Store the selected indices in a new array\n",
    "        model_inputs = input_data[:, train_indices]\n",
    "        model_labels = labels[train_indices]\n",
    "\n",
    "        test_data = input_data[:, test_indices]\n",
    "        test_labels = labels[test_indices]\n",
    "\n",
    "        self.train_indices = train_indices\n",
    "        self.model_inputs = model_inputs\n",
    "        self.input_data = input_data\n",
    "        self.model_labels = model_labels\n",
    "        self.labels = labels\n",
    "        self.indices_with_values = indices_with_values\n",
    "        self.original_shape = original_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "\n",
    "        if self.partition == 'spatial':\n",
    "            val_data = input_data[:, val_indices]\n",
    "            val_labels = labels[val_indices]\n",
    "            self.val_data = val_data\n",
    "            self.val_labels = val_labels\n",
    "\n",
    "    def load_normalize(self, var, var_type='continuous', crop=True):\n",
    "        self.logger.info(f'Loading {var}')\n",
    "        if var == 'landcover' or var == 'NDVI':\n",
    "            feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_japan_flat.npy').astype(np.float32)\n",
    "        elif var == 'precipitation':\n",
    "            feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_daily_japan.npy').astype(np.float32)\n",
    "        elif 'base_model' in var:\n",
    "            feature_data = np.load(f'Output/{self.region}/{var[:-11]}/{self.test}_{var[:-11]}_Susceptibility_base_model.npy').astype(np.float32)\n",
    "            crop = False\n",
    "        elif 'model' in var:\n",
    "            feature_data = np.load(f'Output/{self.region}/{self.hazard}/{self.test}_{self.hazard}_Susceptibility_ensemble_{var}.npy').astype(np.float32)\n",
    "        else:\n",
    "            feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_japan.npy').astype(np.float32)\n",
    "        \n",
    "        if crop:\n",
    "            if self.test == 'hokkaido':\n",
    "                feature_data = feature_data[150:1700,3800:-200]\n",
    "            elif self.test == 'sado':\n",
    "                feature_data = feature_data[2755:2955,3525:3675]\n",
    "        \n",
    "        # factor_x, factor_y = int(feature_data.shape[0] / tile), int(feature_data.shape[1] / tile)\n",
    "        output_shape = feature_data.shape\n",
    "        \n",
    "        # Initialize the scaler, fit and transform the data\n",
    "        if var_type == 'continuous':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled_feature = scaler.fit_transform(feature_data.reshape(-1, 1)).reshape(feature_data.shape)\n",
    "            scaled_feature = np.nan_to_num(scaled_feature, nan=self.missing_data_value)\n",
    "        \n",
    "        elif var_type == 'categorical':\n",
    "            feature_data = np.nan_to_num(feature_data, nan=0)\n",
    "            # Initialize the OneHotEncoder\n",
    "            encoder = LabelEncoder()\n",
    "            # Fit and transform the landcover data\n",
    "            scaled_feature = encoder.fit_transform(feature_data.reshape(-1, 1)).reshape(feature_data.shape)\n",
    "\n",
    "        elif var_type == 'label':\n",
    "            scaled_feature = np.nan_to_num(feature_data, nan=self.missing_data_value)  # Convert nan to a specific value\n",
    "            partition_map = np.load('Region/Japan/japan_prefecture_partitions_with_buffer.npy')\n",
    "            partition_map = partition_map[0:5500,2300:8800]\n",
    "            self.test_prefectures = [2, 6, 16, 10, 18, 34, 43, 39]\n",
    "            self.val_prefectures = [7, 17, 23, 26, 32, 37, 44]\n",
    "            self.train_prefectures = [i for i in range(1, 48) if i not in self.test_prefectures and i not in self.val_prefectures]\n",
    "            spatial_split = []\n",
    "        \n",
    "        elif var_type == 'mask':\n",
    "            scaled_feature = feature_data\n",
    "\n",
    "        # Iterate through the array to extract sub-arrays\n",
    "        scaled_feature_reshape = []\n",
    "        for i in range(self.neighborhood_size, scaled_feature.shape[0] - self.neighborhood_size):\n",
    "            for j in range(self.neighborhood_size, scaled_feature.shape[1] - self.neighborhood_size):\n",
    "                ####### HERE SHOULD BE THE CHECK WITH ELEVATION\n",
    "                sub_array = scaled_feature[i - self.neighborhood_size: i + self.neighborhood_size + 1, j - self.neighborhood_size: j + self.neighborhood_size + 1]\n",
    "                if (var_type == 'label' and var != 'road') | (var == 'multi_hazard') | (self.prep == 'multi' and var_type != 'mask'):\n",
    "                    center_value = sub_array[self.neighborhood_size, self.neighborhood_size]\n",
    "                    scaled_feature_reshape.append(center_value)\n",
    "                    if var_type == 'label':\n",
    "                        if partition_map[i,j] in self.train_prefectures:\n",
    "                            spatial_split.append(1)\n",
    "                        elif partition_map[i,j] in self.val_prefectures:\n",
    "                            spatial_split.append(2)\n",
    "                        elif partition_map[i,j] in self.test_prefectures:\n",
    "                            spatial_split.append(3)\n",
    "                        else:\n",
    "                            spatial_split.append(0)\n",
    "                    # if var == 'HWSD':\n",
    "                    #     print('check')\n",
    "                    #     sys.exit(0)\n",
    "                else:\n",
    "                    scaled_feature_reshape.append(sub_array)\n",
    "\n",
    "        # Convert the list of arrays to a numpy array\n",
    "        scaled_feature_reshape = np.array(scaled_feature_reshape).astype(np.float32)\n",
    "        \n",
    "        # scaled_feature_reshape = scaled_feature.reshape((factor_x * factor_y, int(scaled_feature.shape[0] / factor_x), int(scaled_feature.shape[1] / factor_y), 1))\n",
    "        \n",
    "        if (var_type == 'label' and var != 'road'):\n",
    "            return scaled_feature_reshape.reshape(-1, 1, 1), output_shape, np.array(spatial_split)\n",
    "        elif var == 'multi_hazard':\n",
    "            return scaled_feature_reshape.reshape(-1, 1, 1), output_shape\n",
    "        else:\n",
    "            return np.expand_dims(scaled_feature_reshape, axis=-1)\n",
    "\n",
    "    def train_base_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            if self.hyper:\n",
    "                self.base_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.base_model_instance.run()\n",
    "                self.base_model = self.base_model_instance.base_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "    \n",
    "    # TODO - convert to pyTorch\n",
    "    def xload_base_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            self.base_model = keras.models.load_model(os.path.join(f'Output/{self.region}', self.hazard, f'base_model_{self.test}.tf'))\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "    \n",
    "    def train_ensemble_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            if self.hyper:\n",
    "                self.ensemble_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.ensemble_model_instance.run()\n",
    "                self.combined_model = self.ensemble_model_instance.combined_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "\n",
    "    def train_meta_model(self):\n",
    "        if (self.prep == 'stack') | (self.prep == 'multi'):\n",
    "            if self.hyper:\n",
    "                self.meta_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.meta_model_instance.run()\n",
    "                self.meta_model = self.meta_model_instance.meta_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep=stack | prep=multi')\n",
    "\n",
    "    # TODO - convert to pyTorch\n",
    "    def load_meta_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            self.meta_model = keras.models.load_model(os.path.join(f'Output/{self.region}', self.hazard, f'meta_model_MLP_{self.test}.tf'))\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "\n",
    "    def learning_to_stack(self):\n",
    "        if self.prep == 'model':\n",
    "            self.prep = 'stack'\n",
    "            self.preprocess()\n",
    "        else:\n",
    "            self.logger.info('Only works when prep=model')\n",
    "\n",
    "    def plot(self, data, name='scaled_feature'):\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(data, cmap='viridis')\n",
    "        plt.title(name)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(f'Output/{self.region}/{self.hazard}/{name.replace(\" \", \"_\")}.png', dpi=1000)\n",
    "        return fig\n",
    "\n",
    "    def plot_val_loss(self, history, name='scaled_feature'):\n",
    "        # Visualize the training and validation loss\n",
    "        fig = plt.figure()\n",
    "        plt.plot(history.history['loss'], label='training loss')\n",
    "        plt.plot(history.history['val_loss'], label='validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'Output/{self.region}/{self.hazard}/{name.replace(\" \", \"_\")}.png', dpi=300)\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 16:31:54,128 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_10_16_31_54_2025.log\n",
      "2025-04-10 16:31:54,128 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_10_16_31_54_2025.log\n",
      "2025-04-10 16:31:54,128 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_10_16_31_54_2025.log\n",
      "2025-04-10 16:31:54,128 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_10_16_31_54_2025.log\n",
      "2025-04-10 16:31:54,128 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_10_16_31_54_2025.log\n",
      "2025-04-10 16:31:54,132 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-10 16:31:54,132 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-10 16:31:54,132 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-10 16:31:54,132 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-10 16:31:54,132 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-10 16:31:54,135 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-10 16:31:54,135 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-10 16:31:54,135 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-10 16:31:54,135 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-10 16:31:54,135 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-10 16:31:54,162 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-10 16:31:54,162 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-10 16:31:54,162 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-10 16:31:54,162 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-10 16:31:54,162 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-10 16:31:54,170 - CNN_ls_susc_Europe -INFO - fi:32 ly:3 dv:0.41 lr:0.0001\n",
      "2025-04-10 16:31:54,170 - CNN_ls_susc_Europe -INFO - fi:32 ly:3 dv:0.41 lr:0.0001\n",
      "2025-04-10 16:31:54,170 - CNN_ls_susc_Europe -INFO - fi:32 ly:3 dv:0.41 lr:0.0001\n",
      "2025-04-10 16:31:54,170 - CNN_ls_susc_Europe -INFO - fi:32 ly:3 dv:0.41 lr:0.0001\n",
      "2025-04-10 16:31:54,170 - CNN_ls_susc_Europe -INFO - fi:32 ly:3 dv:0.41 lr:0.0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# sys.exit(0)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     model_instance\u001b[38;5;241m.\u001b[39mtrain_base_model()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     39\u001b[0m     model_instance\u001b[38;5;241m.\u001b[39mload_base_model()\n",
      "Cell \u001b[0;32mIn[38], line 336\u001b[0m, in \u001b[0;36mModelMgr.train_base_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_instance\u001b[38;5;241m.\u001b[39mHypParOpt()\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_instance\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_instance\u001b[38;5;241m.\u001b[39mbase_model\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[37], line 294\u001b[0m, in \u001b[0;36mBaseModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# Set seed\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 294\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    296\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhazard, entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimothy-tiggeloven\u001b[39m\u001b[38;5;124m\"\u001b[39m, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mtest, job_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    297\u001b[0m             config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m    298\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mvariables,\n\u001b[1;32m    305\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39msample_ratio})\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# Develop a basemodel\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "region = 'Europe'\n",
    "#test = 'sado'  # Set test to 'sado', 'hokkaido' or 'japan' as needed\n",
    "hazard = 'Wildfire'  # Set hazard to 'Landslide', 'Flood', 'Tsunami', or 'Multihazard' as needed\n",
    "hyper = 'False'\n",
    "# hyper = True\n",
    "model_choice = 'base'\n",
    "# model_choice = 'lr'\n",
    "\n",
    "# TODO not sure about this\n",
    "# test = sys.argv[1]\n",
    "# hazard = sys.argv[2]\n",
    "# hyper = sys.argv[3]\n",
    "# model_choice = sys.argv[4]\n",
    "\n",
    "if hyper == 'False':\n",
    "    hyper = False\n",
    "else:\n",
    "    hyper = True\n",
    "\n",
    "if model_choice == 'base' or model_choice == 'ensemble':\n",
    "    prep = 'model'\n",
    "elif model_choice == 'meta' and hazard == 'Multihazard':\n",
    "    prep = 'multi'\n",
    "elif model_choice == 'lr':\n",
    "    prep = 'multi'\n",
    "elif model_choice == 'meta':\n",
    "    prep = 'stack'\n",
    "else:\n",
    "    print('Model choice should be base, ensemble or meta')\n",
    "    sys.exit(1)\n",
    "\n",
    "# Instantiate and run the BaseModel\n",
    "model_instance = ModelMgr(hazard=hazard, hyper=hyper, prep=prep, model_choice=model_choice)  # Set test to 'sado' or 'hokkaido' as needed\n",
    "# sys.exit(0)\n",
    "\n",
    "if model_choice == 'base':\n",
    "    model_instance.train_base_model()\n",
    "elif model_choice == 'ensemble':\n",
    "    model_instance.load_base_model()\n",
    "    model_instance.train_ensemble_model()\n",
    "elif model_choice == 'meta' or model_choice == 'lr':\n",
    "    # model_instance.learning_to_stack()\n",
    "    model_instance.train_meta_model()\n",
    "# model_instance.load_meta_model()\n",
    "\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

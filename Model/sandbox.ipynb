{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sympy import primerange\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HazardDataset(Dataset):\n",
    "    def __init__(self, hazard, patch_size=3):\n",
    "        \"\"\"\n",
    "        Custom Dataset for loading hazard-specific features and labels as patches.\n",
    "\n",
    "        Parameters:\n",
    "        - hazard (str): The hazard type (e.g., \"wildfire\").\n",
    "        - patch_size (int): The size of the patch (n x n) around the center cell.\n",
    "        \"\"\"\n",
    "        self.hazard = hazard\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Define the feature file paths for each hazard\n",
    "        self.feature_paths = {\n",
    "            \"Wildfire\": [\n",
    "                \"Input/Europe/downsampled_arrays/masked_NDVI_Europe_flat_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_temperature_daily_Europe_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_landcover_Europe_flat_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_elevation_Europe_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_wind_speed_daily_Europe_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_fire_weather_Europe_downsampled.npy\",\n",
    "                \"Input/Europe/downsampled_arrays/masked_soil_moisture_surface_Europe_downsampled.npy\",\n",
    "            ],\n",
    "        }\n",
    "        self.num_vars = len(self.feature_paths[hazard])\n",
    "\n",
    "        # Define the label file paths for each hazard\n",
    "        self.label_paths = {\n",
    "            \"Wildfire\": \"Input/Europe/downsampled_arrays/masked_wildfire_Europe_downsampled.npy\",\n",
    "        }\n",
    "\n",
    "        # Load the features and labels for the given hazard\n",
    "        if hazard not in self.feature_paths or hazard not in self.label_paths:\n",
    "            raise ValueError(f\"Hazard '{hazard}' is not defined in the dataset.\")\n",
    "\n",
    "        # Load features (stacked along the first axis for channels)\n",
    "        self.features = np.stack([np.load(path).reshape(677,3107) for path in self.feature_paths[hazard]], axis=0)\n",
    "\n",
    "        # Load labels\n",
    "        self.labels = np.load(self.label_paths[hazard]).astype(float).reshape(677,3107)\n",
    "        self.labels = (self.labels > 0).astype(int)  # Binarize labels\n",
    "\n",
    "        # Ensure the spatial dimensions match between features and labels\n",
    "        assert self.features.shape[1:] == self.labels.shape, \"Mismatch between features and labels!\"\n",
    "\n",
    "        # Padding to handle edge cases for patches\n",
    "        self.pad_size = patch_size // 2\n",
    "        self.features = np.pad(\n",
    "            self.features,\n",
    "            pad_width=((0, 0), (self.pad_size, self.pad_size), (self.pad_size, self.pad_size)),  # Correct padding for 3D array\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "        # self.labels = np.pad(\n",
    "        #     self.labels,\n",
    "        #     pad_width=((self.pad_size, self.pad_size), (self.pad_size, self.pad_size)),  # Correct padding for 2D array\n",
    "        #     mode='constant',\n",
    "        #     constant_values=0\n",
    "        # )\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset (total number of cells).\n",
    "        \"\"\"\n",
    "        return self.labels.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single sample (patch and label) at the given index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "        - patch (torch.Tensor): The n x n patch of features.\n",
    "        - label (torch.Tensor): The label for the center cell of the patch.\n",
    "        \"\"\"\n",
    "        # Convert 1D index to 2D spatial index\n",
    "        h, w = self.labels.shape\n",
    "        row, col = divmod(idx, w)\n",
    "\n",
    "        # Extract the patch centered at (row, col)\n",
    "        patch = self.features[:, row:row + self.patch_size, col:col + self.patch_size]\n",
    "        # Get the label for the center cell\n",
    "        label = self.labels[row, col]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        \n",
    "        patch = patch.view(self.num_vars, self.patch_size, self.patch_size)\n",
    "        \n",
    "        return patch, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2103439\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset with a patch size of 5x5\n",
    "hazard = \"Wildfire\"\n",
    "patch_size = 5\n",
    "dataset = HazardDataset(hazard, patch_size=patch_size)\n",
    "\n",
    "# Check the size of the dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get a sample\n",
    "patch, label = dataset[3111]\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, logger, device, num_vars, filters, n_layers, activation, dropout, drop_value, kernel_size, pool_size, neighborhood_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.logger = logger\n",
    "        self.device = device\n",
    "        self.num_vars = num_vars\n",
    "        self.filters = filters\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.drop_value = drop_value\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "\n",
    "        # Define variable-specific blocks\n",
    "        self.var_blocks = nn.ModuleList()\n",
    "        for _ in range(self.num_vars):\n",
    "            layers = [\n",
    "                nn.Conv2d(1, self.filters, kernel_size=self.kernel_size, padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(self.pool_size)\n",
    "            ]\n",
    "            self.var_blocks.append(nn.Sequential(*layers))\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        fc_input_size = self.filters * self.num_vars  # Adjust based on architecture\n",
    "        self.fc1 = nn.Linear(fc_input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.drop_layer = nn.Dropout(self.drop_value) if self.dropout else nn.Identity()\n",
    "        self.output_layer = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Split inputs into a list of tensors, one for each variable\n",
    "        inputs = [inputs[:, i, :, :].unsqueeze(1) for i in range(self.num_vars)]\n",
    "        self.logger.info(f\"Split input shapes: {[inp.shape for inp in inputs]}\")\n",
    "\n",
    "        # Process each variable through its block\n",
    "        features = []\n",
    "        for i, (block, inp) in enumerate(zip(self.var_blocks, inputs)):\n",
    "            x = block(inp)\n",
    "            # self.logger.info(f\"After var_blocks[{i}]: {x.shape}\")\n",
    "            features.append(x)\n",
    "\n",
    "        # Concatenate features along the channel dimension\n",
    "        x = torch.cat(features, dim=1)\n",
    "        self.logger.info(f\"After concatenation: {x.shape}\")\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        self.logger.info(f\"After global_avg_pool: {x.shape}\")\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.logger.info(f\"After flattening: {x.shape}\")\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        self.logger.info(f\"After fc1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        self.logger.info(f\"After bn1: {x.shape}\")\n",
    "        x = self.activation(x)\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        self.logger.info(f\"After output_layer: {x.shape}\")\n",
    "        x = torch.sigmoid(x)\n",
    "        self.logger.info(f\"After sigmoid: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "class BaseModel():\n",
    "    def __init__(self, device, hazard, region, \n",
    "                 variables, neighborhood_size, train_loader, val_loader, logger, seed):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.logger = logger\n",
    "        self.device = device\n",
    "        self.hazard = hazard\n",
    "        self.region = region\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "        self.num_vars = len(variables)\n",
    "        self.variables = variables\n",
    "        self.seed = seed\n",
    "        self.name_model = 'wildfire'\n",
    "\n",
    "\n",
    "        # Data\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        # self.test_loader = ModelMgr_instance.test_loader\n",
    "        \n",
    "    \n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.0001\n",
    "        self.filters = 32\n",
    "        self.n_layers = 3\n",
    "        self.drop_value = 0.41\n",
    "        self.kernel_size = 3\n",
    "        self.pool_size = 2  \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = True\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        # Build the CNN architecture\n",
    "        self.model = self.design_basemodel()\n",
    "        self.model.to(self.device)\n",
    "        # self.logger.info(f\"fi:{self.filters} ly:{self.n_layers} dv:{self.drop_value} lr:{self.learning_rate}\")\n",
    "\n",
    "    def design_basemodel(self):\n",
    "        \"\"\"\n",
    "        Define the CNN architecture in PyTorch.\n",
    "        \"\"\"\n",
    "        self.logger.info('Building architecture')\n",
    "        model = CNN(\n",
    "            logger=self.logger,\n",
    "            device=self.device,\n",
    "            num_vars=self.num_vars,\n",
    "            filters=self.filters,\n",
    "            n_layers=self.n_layers,\n",
    "            activation=self.activation,\n",
    "            dropout=self.dropout,\n",
    "            drop_value=self.drop_value,\n",
    "            kernel_size=self.kernel_size,\n",
    "            pool_size=self.pool_size,\n",
    "            neighborhood_size=self.neighborhood_size\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_binary_crossentropy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Binary cross-entropy loss with NaN handling.\n",
    "        \"\"\"\n",
    "        y_pred = torch.nan_to_num(y_pred, nan=0.0)\n",
    "        y_true = torch.nan_to_num(y_true, nan=0.0)\n",
    "        y_pred = torch.clamp(y_pred, min=1e-7, max=1.0 - 1e-7)\n",
    "        return F.binary_cross_entropy(y_pred, y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_mse(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean squared error (MSE) loss with NaN handling.\n",
    "        \"\"\"\n",
    "        y_pred = torch.nan_to_num(y_pred, nan=0.0)\n",
    "        y_true = torch.nan_to_num(y_true, nan=0.0)\n",
    "        return F.mse_loss(y_pred, y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_mae(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean absolute error (MAE) loss with NaN handling.\n",
    "        \"\"\"\n",
    "        y_pred = torch.nan_to_num(y_pred, nan=0.0)\n",
    "        y_true = torch.nan_to_num(y_true, nan=0.0)\n",
    "        return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model using the provided data loaders.\n",
    "        \"\"\"\n",
    "        # TODO : add early stopping\n",
    "        # TODO : add wandb\n",
    "\n",
    "        \n",
    "        self.logger.info('Training the model')\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        best_val_loss = float('inf')\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(epochs):\n",
    "            self.logger.info(f\"Epoch {epoch+1}/{epochs} starting...\")\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for inputs, labels in self.train_loader:\n",
    "                \n",
    "                self.logger.info(f\"Training batch going through the model\")\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                print(f\"Input shape: {inputs.shape}\")\n",
    "                print(f\"Model device: {next(self.model.parameters()).device}\")\n",
    "                print(f\"Input device: {inputs.device}\")\n",
    "                # Reshape Labels to match output \n",
    "                labels = labels.unsqueeze(1)\n",
    "                print(f\"Label shape: {labels.shape}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                print(f\"Output shape: {outputs.shape}\")\n",
    "                loss = self.safe_binary_crossentropy(labels, outputs)\n",
    "                print(f\"Loss shape: {loss.shape}\")\n",
    "                \n",
    "                self.logger.info(f\"Loss: {loss.item()}\")\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.logger.info(f\"Optimizer step done\")\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                self.logger.info(f\"Train loss: {train_loss}\")\n",
    "\n",
    "            \n",
    "            train_loss /= len(self.train_loader)\n",
    "            val_loss = self.evaluate(self.val_loader)\n",
    "\n",
    "            self.logger.info(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), f\"{self.name_model}_best.pth\")\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the provided data loader.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                self.logger.info(f\"Testing batch going through the model\")\n",
    "\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.safe_binary_crossentropy(labels, outputs)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(data_loader)\n",
    "        return val_loss\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(predictions)\n",
    "    \n",
    "    def testing(self):\n",
    "        \"\"\"\n",
    "        Test the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # TODO rewrite this function\n",
    "\n",
    "        # # Evaluate the model on the validation data\n",
    "        # self.logger.info('Testing')\n",
    "\n",
    "\n",
    "        # y_pred = self.base_model.predict({'input_' + str(i+1): self.ModelMgr_instance.test_data[i] for i in range(len(self.ModelMgr_instance.test_data))})\n",
    "        # y_pred = np.squeeze(y_pred, axis=(1))\n",
    "        # y_true = np.squeeze(self.ModelMgr_instance.test_labels, axis=(1,2))\n",
    "\n",
    "\n",
    "        # #TODO check if torch tensor is needed\n",
    "\n",
    "        # # # Calculate Binary Cross-Entropy\n",
    "        # # y_pred_tf = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        # # y_true_tf = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        # # bce = tf.keras.backend.binary_crossentropy(y_true_tf, y_pred_tf)\n",
    "        # # self.bce_test = tf.reduce_mean(bce).numpy()\n",
    "        # # self.logger.info(f\"BCE: {self.bce_test}\")\n",
    "\n",
    "        # # Metrics\n",
    "        # self.mae = mean_absolute_error(y_true, y_pred)\n",
    "        # self.mse = mean_squared_error(y_true, y_pred)\n",
    "        # self.logger.info(f\"MAE: {self.mae}\")\n",
    "        # self.logger.info(f\"MSE: {self.mse}\")\n",
    "\n",
    "        # # Create a dictionary to store values with names\n",
    "        # metrics_dict = {'MAE': [self.mae], 'MSE': [self.mse]}\n",
    "        # df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "        # # Write the values to a text file\n",
    "        # df.to_csv(f'Output/{self.region}/{self.hazard}/config_{self.ModelMgr_instance.test}_basemodel.csv', index=False)\n",
    "\n",
    "        # # Store in W&B\n",
    "        # wandb.log({\"MAE_test\": self.mae})\n",
    "        # wandb.log({\"MSE_test\": self.mse})\n",
    "        # wandb.log({\"BCE_test\": self.bce_test})\n",
    "\n",
    "    def permutation_feature_importance(self, X, y, baseline_score, metric_fn):\n",
    "        \"\"\"\n",
    "        Compute permutation feature importance.\n",
    "        \"\"\"\n",
    "        feature_importances = []\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            X_permuted = X.clone()\n",
    "            X_permuted[:, i] = X_permuted[torch.randperm(X_permuted.size(0)), i]\n",
    "\n",
    "            permuted_score = metric_fn(y, self.predict(X_permuted))\n",
    "            importance = baseline_score - permuted_score\n",
    "            feature_importances.append(importance)\n",
    "\n",
    "        return feature_importances\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the model training and evaluation.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # TODO wand implementation\n",
    "        self.main()\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Main function to run the model training and evaluation.\n",
    "        \"\"\"\n",
    "        # if self.ModelMgr_instance.hyper:\n",
    "        #     self.base_model = False\n",
    "        #     if self.ModelMgr_instance.partition == 'random':\n",
    "        #         self.ModelMgr_instance.preprocess() \n",
    "        #     wandb.init()\n",
    "        #     self.n_layers = wandb.config.layers\n",
    "        #     self.filters = wandb.config.filters\n",
    "        #     self.learning_rate = wandb.config.lr\n",
    "        #     self.drop_value = wandb.config.dropout\n",
    "\n",
    "        # Develop a basemodel\n",
    "        # self.design_basemodel()\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.logger.info(f\"Optimizer: {optimizer}\")\n",
    "        self.train()\n",
    "        # self.predict()\n",
    "        # self.testing()\n",
    "\n",
    "        # if self.ModelMgr_instance.hyper:\n",
    "        #     new_row = pd.DataFrame([{\n",
    "        #         \"layers\": wandb.config.layers,\n",
    "        #         \"filters\": wandb.config.filters,\n",
    "        #         \"lr\": wandb.config.lr,\n",
    "        #         \"dropout\": wandb.config.dropout,\n",
    "        #         \"val_loss\": self.bce_val,\n",
    "        #         \"MAE\": self.mae,\n",
    "        #         \"MSE\": self.mse,\n",
    "        #     }])\n",
    "        #     self.hyper_df = pd.concat([self.hyper_df, new_row], ignore_index=True)\n",
    "        #     self.hyper_df.to_csv(f\"Output/{self.region}/{self.hazard}/Sweep_results_BaseModel_{self.ModelMgr_instance.test}.csv\", index=False)\n",
    "        #     if self.bce_val < self.bce_val_best:\n",
    "        #         self.bce_val_best = self.bce_val\n",
    "        self.logger.info(f\"Main done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMgr:\n",
    "    def __init__(self, region='Europe', test='Europe', prep='model', hazard='Wildfire', hyper=False, model_choice='base', partition='spatial'):\n",
    "        self.hazard = hazard\n",
    "        self.region = region\n",
    "        self.name_model = 'susceptibility'\n",
    "        self.missing_data_value = 0\n",
    "        self.sample_ratio = 0.8\n",
    "        self.test_split = 0.15\n",
    "        self.neighborhood_size = 5\n",
    "        self.hyper = hyper\n",
    "        self.test = test\n",
    "        self.model_choice = model_choice\n",
    "        self.partition = partition\n",
    "        if self.hazard == 'Landslide':\n",
    "            self.variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux', 'HWSD', 'road', 'GEM', 'curvature', 'GLIM']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous', 'categorical', 'label', 'continuous', 'continuous', 'categorical']\n",
    "            # self.variables = ['elevation', 'slope', 'landcover', 'NDVI', 'precipitation', 'HWSD']\n",
    "            # self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'categorical']\n",
    "        elif self.hazard == 'Flood':\n",
    "            self.variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        elif self.hazard == 'Tsunami':\n",
    "            self.variables = ['elevation', 'coastlines', 'GEM']\n",
    "            self.var_types = ['continuous', 'continuous', 'continuous']\n",
    "            # self.variables = ['coastlines']\n",
    "            # self.var_types = ['continuous']\n",
    "        elif self.hazard == \"Wildfire\":\n",
    "            # temperature_daily, NDVI, landcover, elevation, wind_speed, fire_weather, soil_moisture(root or surface)\n",
    "            self.variables = ['temperature_daily', 'NDVI', 'landcover', 'elevation', 'wind_speed', 'fire_weather', 'root_soil_moisture']\n",
    "            self.var_types = ['continuous', 'continuous', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        elif self.hazard == 'Multihazard':\n",
    "            # self.variables = ['drought', 'extreme_wind', 'fire_weather', 'heatwave', 'pga', 'volcano', 'Flood_base_model', 'Landslide_base_model', 'Tsunami_base_model']\n",
    "            self.variables = ['drought', 'extreme_wind', 'fire_weather', 'heatwave', 'jshis', 'volcano', 'Flood_base_model', 'Landslide_base_model', 'Tsunami_base_model']\n",
    "            self.var_types = ['continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous']\n",
    "        self.prep = prep\n",
    "        self.ensemble_nr = 5  # 5\n",
    "        self.seed = 43\n",
    "\n",
    "        self.logger, self.ch = self.set_logger()\n",
    "\n",
    "\n",
    "\n",
    "       # PyTorch GPU configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        self.logger.info(f\"Using device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            self.logger.info(f\"Num GPUs Available: {torch.cuda.device_count()}\")\n",
    "            self.logger.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        self.logger.info(f\"Torch version: {torch.__version__}\")\n",
    "    \n",
    "        # # Configure memory growth for both GPUs to avoid memory errors\n",
    "        # for gpu in physical_devices:\n",
    "        #     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        # self.logger.info(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "        # Test simple GPU operation\n",
    "        a = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=self.device)\n",
    "        b = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=self.device)\n",
    "        c = torch.matmul(a, b)\n",
    "        self.logger.info(f\"Matrix multiplication result: {c}\")\n",
    "\n",
    "        # sys.exit(0)\n",
    "        \n",
    "\n",
    "        # TODO clean up dataloaders\n",
    "       # Load the dataset\n",
    "        self.logger.info('Loading dataset')\n",
    "        dataset = HazardDataset(hazard=self.hazard, patch_size=self.neighborhood_size)\n",
    "\n",
    "        # Extract labels from the dataset\n",
    "        labels = [dataset[i][1].item() for i in range(len(dataset))]  # Extract labels for stratified sampling\n",
    "\n",
    "        # Perform stratified train-validation split\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(dataset)),\n",
    "            test_size=0.20,  # 20% validation\n",
    "            stratify=labels,  # Ensure class balance\n",
    "            random_state=self.seed\n",
    "        )\n",
    "\n",
    "        # Create subsets using the indices\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "\n",
    "        self.logger.info(f\"Train dataset size before balancing: {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "        train_labels = [dataset[i][1].item() for i in range(len(train_dataset))]  # Extract labels for stratified sampling\n",
    "        train_labels = np.array(train_labels)\n",
    "\n",
    "        # Compute class weights\n",
    "        class_counts = np.bincount(train_labels)\n",
    "        class_weights = 1.0 / class_counts\n",
    "        sample_weights = class_weights[train_labels]\n",
    "\n",
    "        # Create a WeightedRandomSampler\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights)//100,\n",
    "            replacement=True  # Allow replacement to ensure balanced sampling\n",
    "        )\n",
    "\n",
    "        # Create DataLoaders for training and validation\n",
    "        # TODO generalize batch size\n",
    "        batch_size = 32\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.base_model_instance = BaseModel(\n",
    "            device=self.device,\n",
    "            hazard=self.hazard,\n",
    "            region=self.region,\n",
    "            variables=self.variables,\n",
    "            neighborhood_size=self.neighborhood_size,\n",
    "            train_loader=self.train_loader,\n",
    "            val_loader=self.val_loader,\n",
    "            logger=self.logger,\n",
    "            seed=self.seed\n",
    "            \n",
    "        )\n",
    "        # self.ensemble_model_instance = EnsembleModel(self)\n",
    "        # self.meta_model_instance = MetaModel(self)\n",
    "\n",
    "        # if not (self.hyper and self.partition == 'random'):\n",
    "        #     self.preprocess()\n",
    "        # self.preprocess()\n",
    "\n",
    "    def set_logger(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Set-up the logging system, exit if this fails\n",
    "        \"\"\"\n",
    "        # assign logger file name and output directory\n",
    "        datelog = time.ctime()\n",
    "        datelog = datelog.replace(':', '_')\n",
    "        reference = f'CNN_ls_susc_{self.test}'\n",
    "\n",
    "        logfilename = ('logger' + os.sep + reference + '_logfile_' + \n",
    "                    str(datelog.replace(' ', '_')) + '.log')\n",
    "\n",
    "        # create output directory if not exists\n",
    "        if not os.path.exists('logger'):\n",
    "            os.makedirs('logger')\n",
    "\n",
    "        # create logger and set threshold level, report error if fails\n",
    "        try:\n",
    "            logger = logging.getLogger(reference)\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        except IOError:\n",
    "            sys.exit('IOERROR: Failed to initialize logger with: ' + logfilename)\n",
    "\n",
    "        # set formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s -'\n",
    "                                    '%(levelname)s - %(message)s')\n",
    "\n",
    "        # assign logging handler to report to .log file\n",
    "        ch = logging.handlers.RotatingFileHandler(logfilename,\n",
    "                                                maxBytes=10*1024*1024,\n",
    "                                                backupCount=5)\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "\n",
    "        # assign logging handler to report to terminal\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.DEBUG)\n",
    "        console.setFormatter(formatter)\n",
    "        logger.addHandler(console)\n",
    "\n",
    "        # start up log message\n",
    "        logger.info('File logging to ' + logfilename)\n",
    "\n",
    "        return logger, ch \n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Preprocess the data for the model, using the dataset class.\n",
    "\n",
    "        \"\"\"\n",
    "        # set up torch transforms\n",
    "        self.logger.info('Setting up transforms')\n",
    "        transforms = {\n",
    "            'train': torch.nn.Sequential(\n",
    "                # Add any transformations you want here\n",
    "            ),\n",
    "            'val': torch.nn.Sequential(\n",
    "                # Add any transformations you want here\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # TODO check if this is needed\n",
    "    # def preprocess(self):\n",
    "    #     # Load data from .npy files\n",
    "    #     # Prepare data for the CNN\n",
    "    #     input_data = []\n",
    "    #     spatial_split = False\n",
    "    #     if self.prep == 'model':\n",
    "    #         for var, var_type in zip(self.variables, self.var_types):\n",
    "    #             input_data.append(self.load_normalize(var, var_type=var_type))\n",
    "    #     elif self.prep == 'stack':\n",
    "    #         for i in range(self.ensemble_nr):\n",
    "    #             input_data.append(self.load_normalize(f'model_{i}', var_type='label', crop=False)[0])\n",
    "    #     elif self.prep == 'multi':\n",
    "    #         for var, var_type in zip(self.variables, self.var_types):\n",
    "    #             input_data.append(self.load_normalize(var, var_type=var_type))\n",
    "    #     elevation = self.load_normalize('elevation', var_type='mask')\n",
    "    #     input_data = np.array(input_data)\n",
    "\n",
    "    #     if self.hazard == 'Landslide':\n",
    "    #         labels, output_shape, spatial_split = self.load_normalize('ldm', var_type='label')\n",
    "    #     elif self.hazard == 'Flood':\n",
    "    #         labels, output_shape, spatial_split = self.load_normalize('flood_surge', var_type='label')\n",
    "    #     elif self.hazard == 'Tsunami':\n",
    "    #         labels, output_shape, spatial_split = self.load_normalize('tsunami', var_type='label')\n",
    "    #     elif self.hazard == 'Multihazard':\n",
    "    #         labels, output_shape = self.load_normalize('multi_hazard', var_type='continuous')\n",
    "\n",
    "    #     # List to store the indices\n",
    "    #     self.logger.info('Extracting indices')\n",
    "    #     indices_with_values = []\n",
    "    #     original_shape = labels.shape\n",
    "    #     self.logger.info(f\"Input shape: {input_data.shape}\")\n",
    "    #     self.logger.info(f\"Label shape: {labels.shape}\")\n",
    "    #     if spatial_split is not False:\n",
    "    #         self.logger.info(f\"Spatial shape: {spatial_split.shape}\")\n",
    "    #     self.logger.info(f\"Elevation shape: {elevation.shape}\")\n",
    "\n",
    "    #     # Iterate over the array   ############## THIS SHOULD BE DONE IN LOAD NORMALIZE\n",
    "    #     for idx, data_map in enumerate(elevation):\n",
    "    #         if np.any(data_map > -9999): ###### SO WOULD NOT BE BETTER TO CHECK ALL MAPS AND MAKE NODATA=0???? FOR MIN MAX SCALER***\n",
    "    #             indices_with_values.append(idx)\n",
    "\n",
    "    #     # Extract data based on the indices\n",
    "    #     input_data = input_data[:, indices_with_values]\n",
    "    #     labels = labels[indices_with_values]\n",
    "    #     if spatial_split is not False:\n",
    "    #         spatial_split = spatial_split[indices_with_values]\n",
    "\n",
    "    #     self.logger.info(f\"Min value INPUT: {np.min(input_data)}\")\n",
    "    #     self.logger.info(f\"Max value INPUT: {np.max(input_data)}\")\n",
    "    #     self.logger.info(f\"Min value LABEL: {np.min(labels)}\")\n",
    "    #     self.logger.info(f\"Max value LABEL: {np.max(labels)}\")\n",
    "    #     self.logger.info(f\"Input shape: {input_data.shape}\")\n",
    "    #     self.logger.info(f\"Label shape: {labels.shape}\")\n",
    "    #     if spatial_split is not False:\n",
    "    #         self.logger.info(f\"Spatial shape: {spatial_split.shape}\")\n",
    "\n",
    "    #     # for i in range(len(input_data)):\n",
    "    #     #     variables = ['elevation', 'slope', 'landcover', 'aspect', 'NDVI', 'precipitation', 'accuflux', 'HWSD', 'road', 'GEM', 'curvature', 'GLIM']\n",
    "    #     #     self.logger.info(f\"Variable: {variables[i]}\")\n",
    "    #     #     self.logger.info(f\"Min value INPUT: {np.min(input_data[i])}\")\n",
    "    #     #     self.logger.info(f\"Max value INPUT: {np.max(input_data[i])}\")\n",
    "    #     # sys.exit(0)\n",
    "\n",
    "    #     if self.partition == 'random':\n",
    "    #         # Generate random indices from the first axis\n",
    "    #         if not os.path.exists(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy') or self.hyper:\n",
    "    #             train_indices = random.sample(range(input_data.shape[1]), int(input_data.shape[1] * self.sample_ratio))\n",
    "    #             train_indices = np.save(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy', train_indices)\n",
    "\n",
    "    #         train_indices = np.load(f'Output/{self.region}/{self.hazard}/{self.hazard}_Susceptibility_{model_choice}_model_rnd_ind_{self.test}.npy')\n",
    "\n",
    "    #         # Create the test set of indices\n",
    "    #         all_indices = set(range(input_data.shape[1]))\n",
    "    #         complement_indices = list(all_indices - set(train_indices))\n",
    "\n",
    "    #         test_indices = random.sample(complement_indices, int(input_data.shape[1] * self.test_split))\n",
    "    #     elif self.partition == 'spatial':\n",
    "    #         if not os.path.exists(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy'):\n",
    "    #             self.logger.info('INDICES')\n",
    "    #             train_indices = np.where(spatial_split == 1)[0]\n",
    "    #             self.logger.info(train_indices.shape)\n",
    "    #             val_indices = np.where(spatial_split == 2)[0]\n",
    "    #             self.logger.info(val_indices.shape)\n",
    "    #             test_indices = np.where(spatial_split == 3)[0]\n",
    "    #             self.logger.info(test_indices.shape)\n",
    "    #             other_indices = np.where(spatial_split == 0)[0]\n",
    "    #             self.logger.info(other_indices.shape)\n",
    "                \n",
    "    #             train_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy', train_indices)\n",
    "    #             val_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_val.npy', val_indices)\n",
    "    #             test_indices = np.save(f'Output/{self.region}/Susceptibility_spatial_partitioning_test.npy', test_indices)\n",
    "            \n",
    "    #         train_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_train.npy')\n",
    "    #         val_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_val.npy')\n",
    "    #         test_indices = np.load(f'Output/{self.region}/Susceptibility_spatial_partitioning_test.npy')\n",
    "\n",
    "    #     self.input_data = input_data\n",
    "    #     self.labels = labels\n",
    "\n",
    "    #     # Store the selected indices in a new array\n",
    "    #     model_inputs = input_data[:, train_indices]\n",
    "    #     model_labels = labels[train_indices]\n",
    "\n",
    "    #     test_data = input_data[:, test_indices]\n",
    "    #     test_labels = labels[test_indices]\n",
    "\n",
    "    #     self.train_indices = train_indices\n",
    "    #     self.model_inputs = model_inputs\n",
    "    #     self.input_data = input_data\n",
    "    #     self.model_labels = model_labels\n",
    "    #     self.labels = labels\n",
    "    #     self.indices_with_values = indices_with_values\n",
    "    #     self.original_shape = original_shape\n",
    "    #     self.output_shape = output_shape\n",
    "    #     self.test_data = test_data\n",
    "    #     self.test_labels = test_labels\n",
    "\n",
    "    #     if self.partition == 'spatial':\n",
    "    #         val_data = input_data[:, val_indices]\n",
    "    #         val_labels = labels[val_indices]\n",
    "    #         self.val_data = val_data\n",
    "    #         self.val_labels = val_labels\n",
    "\n",
    "    # def load_normalize(self, var, var_type='continuous', crop=True):\n",
    "    #     self.logger.info(f'Loading {var}')\n",
    "    #     if var == 'landcover' or var == 'NDVI':\n",
    "    #         feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_japan_flat.npy').astype(np.float32)\n",
    "    #     elif var == 'precipitation':\n",
    "    #         feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_daily_japan.npy').astype(np.float32)\n",
    "    #     elif 'base_model' in var:\n",
    "    #         feature_data = np.load(f'Output/{self.region}/{var[:-11]}/{self.test}_{var[:-11]}_Susceptibility_base_model.npy').astype(np.float32)\n",
    "    #         crop = False\n",
    "    #     elif 'model' in var:\n",
    "    #         feature_data = np.load(f'Output/{self.region}/{self.hazard}/{self.test}_{self.hazard}_Susceptibility_ensemble_{var}.npy').astype(np.float32)\n",
    "    #     else:\n",
    "    #         feature_data = np.load(f'Input/Japan/npy_arrays/masked_{var}_japan.npy').astype(np.float32)\n",
    "        \n",
    "    #     if crop:\n",
    "    #         if self.test == 'hokkaido':\n",
    "    #             feature_data = feature_data[150:1700,3800:-200]\n",
    "    #         elif self.test == 'sado':\n",
    "    #             feature_data = feature_data[2755:2955,3525:3675]\n",
    "        \n",
    "    #     # factor_x, factor_y = int(feature_data.shape[0] / tile), int(feature_data.shape[1] / tile)\n",
    "    #     output_shape = feature_data.shape\n",
    "        \n",
    "    #     # Initialize the scaler, fit and transform the data\n",
    "    #     if var_type == 'continuous':\n",
    "    #         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #         scaled_feature = scaler.fit_transform(feature_data.reshape(-1, 1)).reshape(feature_data.shape)\n",
    "    #         scaled_feature = np.nan_to_num(scaled_feature, nan=self.missing_data_value)\n",
    "        \n",
    "    #     elif var_type == 'categorical':\n",
    "    #         feature_data = np.nan_to_num(feature_data, nan=0)\n",
    "    #         # Initialize the OneHotEncoder\n",
    "    #         encoder = LabelEncoder()\n",
    "    #         # Fit and transform the landcover data\n",
    "    #         scaled_feature = encoder.fit_transform(feature_data.reshape(-1, 1)).reshape(feature_data.shape)\n",
    "\n",
    "    #     elif var_type == 'label':\n",
    "    #         scaled_feature = np.nan_to_num(feature_data, nan=self.missing_data_value)  # Convert nan to a specific value\n",
    "    #         partition_map = np.load('Region/Japan/japan_prefecture_partitions_with_buffer.npy')\n",
    "    #         partition_map = partition_map[0:5500,2300:8800]\n",
    "    #         self.test_prefectures = [2, 6, 16, 10, 18, 34, 43, 39]\n",
    "    #         self.val_prefectures = [7, 17, 23, 26, 32, 37, 44]\n",
    "    #         self.train_prefectures = [i for i in range(1, 48) if i not in self.test_prefectures and i not in self.val_prefectures]\n",
    "    #         spatial_split = []\n",
    "        \n",
    "    #     elif var_type == 'mask':\n",
    "    #         scaled_feature = feature_data\n",
    "\n",
    "    #     # Iterate through the array to extract sub-arrays\n",
    "    #     scaled_feature_reshape = []\n",
    "    #     for i in range(self.neighborhood_size, scaled_feature.shape[0] - self.neighborhood_size):\n",
    "    #         for j in range(self.neighborhood_size, scaled_feature.shape[1] - self.neighborhood_size):\n",
    "    #             ####### HERE SHOULD BE THE CHECK WITH ELEVATION\n",
    "    #             sub_array = scaled_feature[i - self.neighborhood_size: i + self.neighborhood_size + 1, j - self.neighborhood_size: j + self.neighborhood_size + 1]\n",
    "    #             if (var_type == 'label' and var != 'road') | (var == 'multi_hazard') | (self.prep == 'multi' and var_type != 'mask'):\n",
    "    #                 center_value = sub_array[self.neighborhood_size, self.neighborhood_size]\n",
    "    #                 scaled_feature_reshape.append(center_value)\n",
    "    #                 if var_type == 'label':\n",
    "    #                     if partition_map[i,j] in self.train_prefectures:\n",
    "    #                         spatial_split.append(1)\n",
    "    #                     elif partition_map[i,j] in self.val_prefectures:\n",
    "    #                         spatial_split.append(2)\n",
    "    #                     elif partition_map[i,j] in self.test_prefectures:\n",
    "    #                         spatial_split.append(3)\n",
    "    #                     else:\n",
    "    #                         spatial_split.append(0)\n",
    "    #                 # if var == 'HWSD':\n",
    "    #                 #     print('check')\n",
    "    #                 #     sys.exit(0)\n",
    "    #             else:\n",
    "    #                 scaled_feature_reshape.append(sub_array)\n",
    "\n",
    "    #     # Convert the list of arrays to a numpy array\n",
    "    #     scaled_feature_reshape = np.array(scaled_feature_reshape).astype(np.float32)\n",
    "        \n",
    "    #     # scaled_feature_reshape = scaled_feature.reshape((factor_x * factor_y, int(scaled_feature.shape[0] / factor_x), int(scaled_feature.shape[1] / factor_y), 1))\n",
    "        \n",
    "    #     if (var_type == 'label' and var != 'road'):\n",
    "    #         return scaled_feature_reshape.reshape(-1, 1, 1), output_shape, np.array(spatial_split)\n",
    "    #     elif var == 'multi_hazard':\n",
    "    #         return scaled_feature_reshape.reshape(-1, 1, 1), output_shape\n",
    "    #     else:\n",
    "    #         return np.expand_dims(scaled_feature_reshape, axis=-1)\n",
    "\n",
    "    def train_base_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            if self.hyper:\n",
    "                self.base_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.logger.info('Training base model')\n",
    "                self.base_model_instance.run()\n",
    "                self.base_model = self.base_model_instance.base_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "    \n",
    "    # TODO - convert to pyTorch\n",
    "    def xload_base_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            self.base_model = keras.models.load_model(os.path.join(f'Output/{self.region}', self.hazard, f'base_model_{self.test}.tf'))\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "    \n",
    "    def train_ensemble_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            if self.hyper:\n",
    "                self.ensemble_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.ensemble_model_instance.run()\n",
    "                self.combined_model = self.ensemble_model_instance.combined_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "\n",
    "    def train_meta_model(self):\n",
    "        if (self.prep == 'stack') | (self.prep == 'multi'):\n",
    "            if self.hyper:\n",
    "                self.meta_model_instance.HypParOpt()\n",
    "            else:\n",
    "                self.meta_model_instance.run()\n",
    "                self.meta_model = self.meta_model_instance.meta_model\n",
    "        else:\n",
    "            self.logger.info('Only works when prep=stack | prep=multi')\n",
    "\n",
    "    # TODO - convert to pyTorch\n",
    "    def load_meta_model(self):\n",
    "        if self.prep != 'stack':\n",
    "            self.meta_model = keras.models.load_model(os.path.join(f'Output/{self.region}', self.hazard, f'meta_model_MLP_{self.test}.tf'))\n",
    "        else:\n",
    "            self.logger.info('Only works when prep!=stack')\n",
    "\n",
    "    def learning_to_stack(self):\n",
    "        if self.prep == 'model':\n",
    "            self.prep = 'stack'\n",
    "            self.preprocess()\n",
    "        else:\n",
    "            self.logger.info('Only works when prep=model')\n",
    "\n",
    "    def plot(self, data, name='scaled_feature'):\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(data, cmap='viridis')\n",
    "        plt.title(name)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(f'Output/{self.region}/{self.hazard}/{name.replace(\" \", \"_\")}.png', dpi=1000)\n",
    "        return fig\n",
    "\n",
    "    def plot_val_loss(self, history, name='scaled_feature'):\n",
    "        # Visualize the training and validation loss\n",
    "        fig = plt.figure()\n",
    "        plt.plot(history.history['loss'], label='training loss')\n",
    "        plt.plot(history.history['val_loss'], label='validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'Output/{self.region}/{self.hazard}/{name.replace(\" \", \"_\")}.png', dpi=300)\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 11:49:03,481 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Fri_May__2_11_49_03_2025.log\n",
      "2025-05-02 11:49:03,503 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-05-02 11:49:03,504 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-05-02 11:49:05,689 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-05-02 11:49:05,690 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-05-02 11:49:28,833 - CNN_ls_susc_Europe -INFO - Train dataset size before balancing: 1682751\n",
      "2025-05-02 11:49:46,412 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-05-02 11:49:46,454 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-05-02 11:49:46,678 - CNN_ls_susc_Europe -INFO - Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "2025-05-02 11:49:46,678 - CNN_ls_susc_Europe -INFO - Training the model\n",
      "2025-05-02 11:49:46,690 - CNN_ls_susc_Europe -INFO - Epoch 1/10 starting...\n",
      "2025-05-02 11:49:46,692 - CNN_ls_susc_Europe -INFO - Training batch going through the model\n",
      "2025-05-02 11:49:46,693 - CNN_ls_susc_Europe -INFO - Split input shapes: [torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5]), torch.Size([32, 1, 5, 5])]\n"
     ]
    }
   ],
   "source": [
    "model_manager = ModelMgr(hazard='Wildfire')\n",
    "model_manager.train_base_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,392 - CNN_ls_susc_Europe -INFO - File logging to logger/CNN_ls_susc_Europe_logfile_Thu_Apr_24_19_19_00_2025.log\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,397 - CNN_ls_susc_Europe -INFO - Using device: mps\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,402 - CNN_ls_susc_Europe -INFO - Torch version: 2.2.2\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,416 - CNN_ls_susc_Europe -INFO - Matrix multiplication result: tensor([[19., 22.],\n",
      "        [43., 50.]], device='mps:0')\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,422 - CNN_ls_susc_Europe -INFO - Loading dataset\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,481 - CNN_ls_susc_Europe -INFO - Building architecture\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,486 - CNN_ls_susc_Europe -INFO - Initializing CNN model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n",
      "2025-04-24 19:19:00,518 - CNN_ls_susc_Europe -INFO - Training base model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModel' object has no attribute 'ModelMgr_instance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# sys.exit(0)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     model_instance\u001b[38;5;241m.\u001b[39mtrain_base_model()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     39\u001b[0m     model_instance\u001b[38;5;241m.\u001b[39mload_base_model()\n",
      "Cell \u001b[0;32mIn[49], line 400\u001b[0m, in \u001b[0;36mModelMgr.train_base_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining base model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_instance\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_instance\u001b[38;5;241m.\u001b[39mbase_model\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[46], line 222\u001b[0m, in \u001b[0;36mBaseModel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Run the model training and evaluation.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    223\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mModelMgr_instance\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# TODO wand implementation\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MasterThesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModel' object has no attribute 'ModelMgr_instance'"
     ]
    }
   ],
   "source": [
    "\n",
    "region = 'Europe'\n",
    "#test = 'sado'  # Set test to 'sado', 'hokkaido' or 'japan' as needed\n",
    "hazard = 'Wildfire'  # Set hazard to 'Landslide', 'Flood', 'Tsunami', or 'Multihazard' as needed\n",
    "hyper = 'False'\n",
    "# hyper = True\n",
    "model_choice = 'base'\n",
    "# model_choice = 'lr'\n",
    "\n",
    "# TODO not sure about this\n",
    "# test = sys.argv[1]\n",
    "# hazard = sys.argv[2]\n",
    "# hyper = sys.argv[3]\n",
    "# model_choice = sys.argv[4]\n",
    "\n",
    "if hyper == 'False':\n",
    "    hyper = False\n",
    "else:\n",
    "    hyper = True\n",
    "\n",
    "if model_choice == 'base' or model_choice == 'ensemble':\n",
    "    prep = 'model'\n",
    "elif model_choice == 'meta' and hazard == 'Multihazard':\n",
    "    prep = 'multi'\n",
    "elif model_choice == 'lr':\n",
    "    prep = 'multi'\n",
    "elif model_choice == 'meta':\n",
    "    prep = 'stack'\n",
    "else:\n",
    "    print('Model choice should be base, ensemble or meta')\n",
    "    sys.exit(1)\n",
    "\n",
    "# Instantiate and run the BaseModel\n",
    "model_instance = ModelMgr(hazard=hazard, hyper=hyper, prep=prep, model_choice=model_choice)  # Set test to 'sado' or 'hokkaido' as needed\n",
    "# sys.exit(0)\n",
    "\n",
    "if model_choice == 'base':\n",
    "    model_instance.train_base_model()\n",
    "elif model_choice == 'ensemble':\n",
    "    model_instance.load_base_model()\n",
    "    model_instance.train_ensemble_model()\n",
    "elif model_choice == 'meta' or model_choice == 'lr':\n",
    "    # model_instance.learning_to_stack()\n",
    "    model_instance.train_meta_model()\n",
    "# model_instance.load_meta_model()\n",
    "\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
